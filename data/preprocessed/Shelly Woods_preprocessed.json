{
  "id": "Shelly Woods",
  "raw_text": "Shelly Woods\n(201) 203-3169\nProfessional Summary\n10+ years of professional experience in IT on JAVA, JEE including 2+ years of hands on experience in Big Data, Hadoop Ecosystem Components.\nExpertise in developing application with financial domain, using Enterprise Technologies pertaining to  Core Java 1.8 , JEE , Servlets 2.2/2.3, JSP 2.0, Struts 2.0, Hibernate 3.0, Spring IOC, Spring MVC, Spring Boot Hibernate, JMS,XML, JDBC 2.0, JNDI, JAXP ,JAXB, Web Logic ,Web Sphere and Tomcat.\nExperience in Web Services using XML, HTML, SOAP and REST API.\nSolid background in Object Oriented Analysis & Design, Development and Implementation of Client Server/Web/Enterprise development using n-tier architecture\nknowledge of Angular JS practices ,Creating custom, general use modules and components which extend the elements and modules of core Angular JS\nStrong experience creating real time data streaming solutions using Apache Spark Core, Spark SQL & Data Frames, Spark Streaming \nIn depth knowledge of Hadoop Architecture and YARN\nExperience in writing Map Reduce programs using Apache Hadoop for analyzing Big Data.\nHands on experience in writing Ad-hoc Queries for moving data from HDFS to HIVE and analyzing the data using HIVE QL.\nExperience in importing and exporting data using Sqoop from Relational Database Systems to HDFS.\nExperience in writing Hadoop Jobs for analyzing data using Pig Latin.\nWorking Knowledge in NoSQL Databases like HBase.\nIntegrated Apache Kafka for data ingestion\nExperience in using Apache Flume for collecting, aggregating and moving large amounts of data from application servers.\nExperience in using Zookeeper and Oozie Operational Services for coordinating the cluster and scheduling workflows.\nExtensive experience with SQL, PL/SQL, Shell Scripting and database concepts.\nExperience in using version control management tools like CVS, SVN and Rational Clear Case.\nHighly motivated, self-starter with a positive attitude, willingness to learn new concepts and acceptance of challenges.\nAbility to work independently and with a group of peers in a results-driven environment. Strong analytical and problem solving skills. Ability to take initiative and learn emerging technologies and programming languages\nEducation:\nM.C.A (Master of Computer Applications) from Osmania University, 2006.\nB.S.C (Computer Science) from Kakatiya University, 2003.\nTechnical Skills:\t\t\nProfessional Experience:\nClient: \tAT&T, USA\t\t                                                                                               April 2017-Persent.\t\nTitle: Senior Analytical Hadoop / Spark Developer\nECOMP is critical in achieving AT&T’s D2 imperatives to increase the value of our network to customers by rapidly on-boarding new services (created by AT&T or 3rd parties), enabling the creation of a new ecosystem of cloud consumer and enterprise services, reducing Capital and Operational Expenditures, and providing Operations efficiencies. It delivers enhanced customer experience by allowing them in near real time to reconfigure their network, services, and capacity. While ECOMP does not directly support legacy physical elements, it works with traditional OSS’s to provide a seamless customer experience across both virtual and physical elements. ECOMP enables network agility, elasticity, and improves Time-to-Market/Revenue/Scale via the AT&T Service Design and Creation (ASDC) visual modeling and design. The service design and creation capabilities and policy recipes eliminate many of the manual and long running processes performed via traditional OSS’s (e.g., break-fix largely moves to plan and build function). The ECOMP platform provides external applications (OSS/BSS, customer apps, and 3rd party integration) with a secured, RESTful API access control to ECOMP services.\nResponsibilities:\nDeveloped Spark scripts by using Scala shell commands as per the requirement.\nUsed Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.\nDeveloped Scala scripts, UDFFs using both Data frames/SQL and RDD/MapReduce in Spark  for Data Aggregation, queries and writing data back into OLTP system through Sqoop.\nExperienced in performance tuning of Spark Applications for setting right Batch Interval time, correct level of Parallelism and memory tuning.\nLoaded the data into Spark RDD and do in memory data Computation to generate the Output response.\nOptimizing of existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frames and Pair RDD's.\nInvolved in loading data from Oracle database into HDFS using Sqoop queries.\nResponsible for building scalable distributed data solutions using Hadoop.\nDeveloped Map Reduce pipeline jobs to process the data and create necessary HFiles. \nDeveloped Pig Latin scripts for data cleansing.\nEnvironment: HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT 0.13, Oozie, kafka-0.9.0, Apache NiFi ,Java (jdk1.7 & 1.8 ), UNIX, SVN and Zookeeper, JEE, JSP, JSTL,  Spring 2.5, Oracle 11g/10g,Maven, REST-ful Web Services,  Apache Axis2, LINUX,  Tomcat7,GIt,Jenikins.\nClient: \tAmerican Express, USA\t\t                               August 2016-March2017.\t\nTitle: Senior Java/Hadoop Developer\nProject Description:\nAmerican Express Company is a global travel, financial and network service provider. The company provides individuals with charge and credit cards. In CCSG we build new application pages for the given cards (Charge/Credit). Enables prospect to apply for personal cards. Prospect has an option to apply for a personal card through various channels.\nLong Application: User is not an American Express card holder and wants to apply for a new card.\nShort Application: If already a card member, has an option to apply for new cards, with a difference that user is asked to fill in minimal details\nResponsibilities: \nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle\nInvolved in loading data from Oracle database into HDFS using Sqoop queries.\nResponsible for building scalable distributed data solutions using Hadoop.\nDeveloped Map Reduce pipeline jobs to process the data and create necessary HFiles. \nDeveloped Pig Latin scripts for data cleansing.\nWorked with different File Formats like TEXTFILE, AVROFILE for HIVE querying and processing.\n Developed PIG UDF'S for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders.\nI have successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using Yardstick framework to measure performance of Apache Ignite Streaming and Apache Spark Streaming. \nUsing Apache Nifi to Stream data Feeds to Kafka.\nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle.\nImplemented test cases for Spark using Scala as language. \nEnvironment: Hadoop, Map Reduce, HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT 0.13, Oozie, kafka-0.9.0, Apache NiFi ,Java (jdk1.7 & 1.8 ), UNIX, SVN and Zookeeper, JEE, JSP, JSTL,  Spring 2.5, Oracle 11g/10g,Maven, REST-ful Web Services, SOAP, Apache Axis2, LINUX,  Tomcat7\nClient: \tUnited Overseas Bank, Singapore\t\t\t\tSeptember 2014- August 2016\nTitle:   Tech Lead( Java/Hadoop)\nDescription: \t         \nUOB, The Power Lender CE is a loan origination system supporting all kinds of loans. The Main objective of this project is to build an interface to interact with External Fraud Detection System for both Secured and Unsecured Loans. This system consists of two basic modules such as Secured Loans are mortgage loans, housing loans or any loan that is availed form the bank on providing some security or collateral. Unsecured Loans are credit cards, personal loans, vehicle loans or any loan that is availed from the bank without any security. \nResponsibilities:\nCollaborated with the Business Intelligence team to understand the high level data roadmap and define data discovery priorities.\nInstalled and configured Hadoop-1.0.2 Map Reduce, HDFS, and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing. \nAnalyzed Hadoop cluster using the big data analytical tools such as Pig, Hive, Scoop and Flume. \nCollected and aggregated large amounts of web log data from different sources such as web servers, mobile and network devices using Apache Flume and stored the data into HDFS for analysis.\nDeveloped optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop.\nInvolved in creating Hive tables, loading millions of records of the stored log data and writing queries that will invoke and run the Map Reduce jobs in the backend. \nTransformed large sets of semi-structured and unstructured data in various formats, to extract parameters such as user location, age, spending time etc.\nAnalyzed the web log data using Hive to calculate metrics such as number of unique visitors, page views, etc.\nExported the analyzed data to relational databases using Sqoop for visualization and generating reports.\nDesigned efficient high-performing applications to extract, transform, load, and query very large datasets, including unstructured data. \nInstalled Apache Oozie workflow engine to run multiple Hive and Pig jobs independently with time and data availability.\nModelled user behavior based upon previous findings and most relevant data available, and contributed to the development of tools for tracking and understanding user behavior.\nWorked on Hive joins to produce the input data set.\nEnvironment: Hadoop, Map Reduce, HDFS, Hive, Oracle 11g/10g, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT  0.13 ,Oozie, Java (jdk1.6), UNIX, SVN and Zookeeper,Java, J2EE, JSP, JSTL, AngularJS, Spring 2.5,Maven, REST-ful Web Services, SOAP, Apache Axis2, LINUX,  Tomcat7,Maven\nUniversal Weather & Aviation.                                        July 2010 - August 2014\t \nTitle: Senior Java Developer\nDescription:\t          \nThe application will provide enhanced electronic versions of Universal's product UVTripPlanner. It will provide search facility for ground services and airport data. This is the web application provides information about weather and aviation details for registered users across the globe. User can get the data to every single airport includes airport address, frequencies, customs, and services available for different types of aircrafts.  \nResponsibilities: \nUsed Agile Methodologies to manage full life-cycle development of the project.\nDeveloped application using Struts, spring and Hibernate.\nDeveloped rich user interface using JavaScript, JSTL, CSS, JQuery and JSP’s.\nDeveloped custom tags for implementing logic in JSP’s.\nUsed Java script, JQuery, JSTL, CSS and Struts 2 tags for developing the JSP’S.\nInvolved in making release builds for deploying the application for test environments.\nUsed Oracle database as backend database.\nWrote SQL to update and create database tables.\nUsed Eclipse as IDE.\nUsing RIDC Interface get content details and Create Content through application.\nUsed Spring IOC for injecting the beans.\nUsed Hibernate for connecting to the database and mapping the entities by using hibernate annotations.\nCreated JUnit test cases for unit testing application.\nWriting/integration JSP communicating to spring controller  and passing query criteria to hibernate to pull data  and showing reports based on searches both on web gui and streaming data into excel .\nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle.\nDevelopment Tools are Maven, Spring MVC,Hibernate\nAppserversIwebservers:Tomcat7\nUsed JUNIT and JMOCK for unit testing.\nEnvironment: J2EE1.6, JSP, JSTL, Ajax, Spring 2.5, Struts 2.0, Ajax, Hibernate 3.2,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log4j, ORACLE 11g, Oracle Web logic Server 10.3, SVN, Windows XP, UML.\nHoneywell                                                                                                 December 2009- July 2010\nJava Developer\nDescription:     The Honeywell_AP process covers processing of all vendor related payments for clients, which includes processing of invoices, credit notes and payment requests etc. On an overall basis, timely and accurate payments with complete controllership and diligent customer service are the required deliverables of AP\n\nResponsibilities: \nInvolved in the requirements gathering. Design, Development, Unit testing and Bug fixing.\nUsed Agile Methodologies to manage full life-cycle development of the project.\nInvolved in development of GUI for Client Using Java Swing Development using Java1.6, Java swing worker also developed the entire online help for the application system using the Java help system, the application had perspectives for views to support multiple tab Components in the UI, it was packaged as a web start build.\nInvolved in creating ant scripts for the Jnlp files to update client builds for the Usability tests.\nWorked with Generics using Java1.6 and some other open source tools to build the C2C1M Client like, Table Layout, Jakarta P01etc, Worked with JDIC (Java Desktop integration Components) components to embed a web browser displaying HTML in a Java window.\nWorked on JMS to publish and subscribe to topics messages downstream to update legacy data using IBM MQ   series. \nCreated/Configured Hibernate mapping classes, 1-Hibernate configuration files (XML files) to use the Hibernate frame work to update or view Database records, also created Criteria Queries to retrieve data from the Database.\nDeveloped RMT servers of client and server could communicate for data updates and retrievals. The combination of hibernate with RMI was simple and perfect solution for concurrency issues.\nSoftware Development Methodology used to develop the project was Agile Methodology (with small scrum teams).Connected client to web service (JAX—RPC) to retrieve data and d display for central storage, the application was developed over (lie spring framework.)\nWorked on writing/updating ANT scripts while creating web archive files/enterprise archive files the jars were signed to maintain security for the systems.\nBuilt high performance java servers and used maven for building/packaging.\nDesign of the Client side of the application using Java swing (1.6) using the JSR296 application framework.\nCollaborating with analysis team to review the developed UI and packaging/deployment the application for UAT and SIT using java web start build techniques.\nDevelopment tools: JavaSwing,Intellijidea,YourKit Profiler \nApplication server: Websphere\nServer technology: RMI Remote Method Invocation), Hibernate. Version Control;Clearcase\nSpecial tools; JDIC, Table Layout (opensourceswing’s layout), Glazed Lists, lntelliJ IDEA, Your Kit Java Profiler (to clear memory leaks in the client/server code), Deadlocks P0I (Apache for exporting to excel from the GUI tables), hibernate..\nEnvironment: J2EE1.6, JSP, JSTL, Ajax, Spring 2.5, Struts 2.0, Ajax, Hibernate 3.2,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log4j, ORACLE 11g, Oracle Web logic Server 10.3, SVN, Windows XP, UML, Sterling Commerce Distributed Order Management(DOM).\nSpecialty Planners Inc                                                                           November 2007- December 2009\nJava/J2EE Developer\nDescription:  \nSpecialty Planners Inc. is an Insurance Agency sales company that sells Long Term Care (LTC) products of various Insurance companies (called Carriers) and gets commission for the business done, from the Carriers. Agents are recruited and trained by Specialty Planners to do business for the Carriers and the Carrier takes agent into contract. Specialty Planners supplies leads (prospect customer information) to its Agents via AIMS IC online application for doing business. The main objective of the AIMS application is that provides support and management for the Specialty Planners business. The system will allow Specialty Planners to operate efficiently and support brokerage business model and third party administration business.\nResponsibilities:\nInvolved in Unit Testing of the Application.\nDeveloped the JSP’s and Modeling data using MVC architecture.\nInvolved in designing front-end screens.\nDocumenting daily weekly status report and sending to client\nImplemented the Servlets and JSP components\nUsed Hibernate for Object Relational Mapping and data persistence.\n Developed the Database interaction classes using JDBC.\nCreated JUnit test cases and ANT scripts for build automation.\nEnvironment: Java, J2EE 1.4, HTML, XML, JDBC, JMS,  Servlets, JSP 1.2, Struts 1.2, Hibernate, Web services, Eclipse 3.3, Web Sphere 7,  Oracle 9i, ANT, Microsoft Visio.",
  "cleaned_text": "Shelly Woods Professional Summary years of professional experience in IT on JAVA, JEE including years of hands on experience in Big Data, Hadoop Ecosystem Components. Expertise in developing application with financial domain, using Enterprise Technologies pertaining to Core Java . , JEE , Servlets . . , JSP . , Struts . , Hibernate . , Spring IOC, Spring MVC, Spring Boot Hibernate, JMS,XML, JDBC . , JNDI, JAXP ,JAXB, Web Logic ,Web Sphere and Tomcat. Experience in Web Services using XML, HTML, SOAP and REST API. Solid background in Object Oriented Analysis & Design, Development and Implementation of Client Server Web Enterprise development using n-tier architecture knowledge of Angular JS practices ,Creating custom, general use modules and components which extend the elements and modules of core Angular JS Strong experience creating real time data streaming solutions using Apache Spark Core, Spark SQL & Data Frames, Spark Streaming In depth knowledge of Hadoop Architecture and YARN Experience in writing Map Reduce programs using Apache Hadoop for analyzing Big Data. Hands on experience in writing Ad-hoc Queries for moving data from HDFS to HIVE and analyzing the data using HIVE QL. Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS. Experience in writing Hadoop Jobs for analyzing data using Pig Latin. Working Knowledge in NoSQL Databases like HBase. Integrated Apache Kafka for data ingestion Experience in using Apache Flume for collecting, aggregating and moving large amounts of data from application servers. Experience in using Zookeeper and Oozie Operational Services for coordinating the cluster and scheduling workflows. Extensive experience with SQL, PL SQL, Shell Scripting and database concepts. Experience in using version control management tools like CVS, SVN and Rational Clear Case. Highly motivated, self-starter with a positive attitude, willingness to learn new concepts and acceptance of challenges. Ability to work independently and with a group of peers in a results-driven environment. Strong analytical and problem solving skills. Ability to take initiative and learn emerging technologies and programming languages Education M.C.A Master of Computer Applications from Osmania University, . B.S.C Computer Science from Kakatiya University, . Technical Skills Professional Experience Client AT&T, USA April -Persent. Title Senior Analytical Hadoop Spark Developer ECOMP is critical in achieving AT&Ts D imperatives to increase the value of our network to customers by rapidly on-boarding new services created by AT&T or rd parties , enabling the creation of a new ecosystem of cloud consumer and enterprise services, reducing Capital and Operational Expenditures, and providing Operations efficiencies. It delivers enhanced customer experience by allowing them in near real time to reconfigure their network, services, and capacity. While ECOMP does not directly support legacy physical elements, it works with traditional OSSs to provide a seamless customer experience across both virtual and physical elements. ECOMP enables network agility, elasticity, and improves Time-to-Market Revenue Scale via the AT&T Service Design and Creation ASDC visual modeling and design. The service design and creation capabilities and policy recipes eliminate many of the manual and long running processes performed via traditional OSSs e.g., break-fix largely moves to plan and build function . The ECOMP platform provides external applications OSS BSS, customer apps, and rd party integration with a secured, RESTful API access control to ECOMP services. Responsibilities Developed Spark scripts by using Scala shell commands as per the requirement. Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive. Developed Scala scripts, UDFFs using both Data frames SQL and RDD MapReduce in Spark for Data Aggregation, queries and writing data back into OLTP system through Sqoop. Experienced in performance tuning of Spark Applications for setting right Batch Interval time, correct level of Parallelism and memory tuning. Loaded the data into Spark RDD and do in memory data Computation to generate the Output response. Optimizing of existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frames and Pair RDD s. Involved in loading data from Oracle database into HDFS using Sqoop queries. Responsible for building scalable distributed data solutions using Hadoop. Developed Map Reduce pipeline jobs to process the data and create necessary HFiles. Developed Pig Latin scripts for data cleansing. Environment HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT . , Oozie, kafka- . . , Apache NiFi ,Java jdk . & . , UNIX, SVN and Zookeeper, JEE, JSP, JSTL, Spring . , Oracle g g,Maven, REST-ful Web Services, Apache Axis , LINUX, Tomcat ,GIt,Jenikins. Client American Express, USA August -March . Title Senior Java Hadoop Developer Project Description American Express Company is a global travel, financial and network service provider. The company provides individuals with charge and credit cards. In CCSG we build new application pages for the given cards Charge Credit . Enables prospect to apply for personal cards. Prospect has an option to apply for a personal card through various channels. Long Application User is not an American Express card holder and wants to apply for a new card. Short Application If already a card member, has an option to apply for new cards, with a difference that user is asked to fill in minimal details Responsibilities Involved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle Involved in loading data from Oracle database into HDFS using Sqoop queries. Responsible for building scalable distributed data solutions using Hadoop. Developed Map Reduce pipeline jobs to process the data and create necessary HFiles. Developed Pig Latin scripts for data cleansing. Worked with different File Formats like TEXTFILE, AVROFILE for HIVE querying and processing. Developed PIG UDF S for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders. I have successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using Yardstick framework to measure performance of Apache Ignite Streaming and Apache Spark Streaming. Using Apache Nifi to Stream data Feeds to Kafka. Involved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle. Implemented test cases for Spark using Scala as language. Environment Hadoop, Map Reduce, HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT . , Oozie, kafka- . . , Apache NiFi ,Java jdk . & . , UNIX, SVN and Zookeeper, JEE, JSP, JSTL, Spring . , Oracle g g,Maven, REST-ful Web Services, SOAP, Apache Axis , LINUX, Tomcat Client United Overseas Bank, Singapore September - August Title Tech Lead Java Hadoop Description UOB, The Power Lender CE is a loan origination system supporting all kinds of loans. The Main objective of this project is to build an interface to interact with External Fraud Detection System for both Secured and Unsecured Loans. This system consists of two basic modules such as Secured Loans are mortgage loans, housing loans or any loan that is availed form the bank on providing some security or collateral. Unsecured Loans are credit cards, personal loans, vehicle loans or any loan that is availed from the bank without any security. Responsibilities Collaborated with the Business Intelligence team to understand the high level data roadmap and define data discovery priorities. Installed and configured Hadoop- . . Map Reduce, HDFS, and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing. Analyzed Hadoop cluster using the big data analytical tools such as Pig, Hive, Scoop and Flume. Collected and aggregated large amounts of web log data from different sources such as web servers, mobile and network devices using Apache Flume and stored the data into HDFS for analysis. Developed optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop. Involved in creating Hive tables, loading millions of records of the stored log data and writing queries that will invoke and run the Map Reduce jobs in the backend. Transformed large sets of semi-structured and unstructured data in various formats, to extract parameters such as user location, age, spending time etc. Analyzed the web log data using Hive to calculate metrics such as number of unique visitors, page views, etc. Exported the analyzed data to relational databases using Sqoop for visualization and generating reports. Designed efficient high-performing applications to extract, transform, load, and query very large datasets, including unstructured data. Installed Apache Oozie workflow engine to run multiple Hive and Pig jobs independently with time and data availability. Modelled user behavior based upon previous findings and most relevant data available, and contributed to the development of tools for tracking and understanding user behavior. Worked on Hive joins to produce the input data set. Environment Hadoop, Map Reduce, HDFS, Hive, Oracle g g, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT . ,Oozie, Java jdk . , UNIX, SVN and Zookeeper,Java, J EE, JSP, JSTL, AngularJS, Spring . ,Maven, REST-ful Web Services, SOAP, Apache Axis , LINUX, Tomcat ,Maven Universal Weather & Aviation. July - August Title Senior Java Developer Description The application will provide enhanced electronic versions of Universal s product UVTripPlanner. It will provide search facility for ground services and airport data. This is the web application provides information about weather and aviation details for registered users across the globe. User can get the data to every single airport includes airport address, frequencies, customs, and services available for different types of aircrafts. Responsibilities Used Agile Methodologies to manage full life-cycle development of the project. Developed application using Struts, spring and Hibernate. Developed rich user interface using JavaScript, JSTL, CSS, JQuery and JSPs. Developed custom tags for implementing logic in JSPs. Used Java script, JQuery, JSTL, CSS and Struts tags for developing the JSPS. Involved in making release builds for deploying the application for test environments. Used Oracle database as backend database. Wrote SQL to update and create database tables. Used Eclipse as IDE. Using RIDC Interface get content details and Create Content through application. Used Spring IOC for injecting the beans. Used Hibernate for connecting to the database and mapping the entities by using hibernate annotations. Created JUnit test cases for unit testing application. Writing integration JSP communicating to spring controller and passing query criteria to hibernate to pull data and showing reports based on searches both on web gui and streaming data into excel . Involved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle. Development Tools are Maven, Spring MVC,Hibernate AppserversIwebservers Tomcat Used JUNIT and JMOCK for unit testing. Environment J EE . , JSP, JSTL, Ajax, Spring . , Struts . , Ajax, Hibernate . ,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log j, ORACLE g, Oracle Web logic Server . , SVN, Windows XP, UML. Honeywell December - July Java Developer Description The Honeywell AP process covers processing of all vendor related payments for clients, which includes processing of invoices, credit notes and payment requests etc. On an overall basis, timely and accurate payments with complete controllership and diligent customer service are the required deliverables of AP Responsibilities Involved in the requirements gathering. Design, Development, Unit testing and Bug fixing. Used Agile Methodologies to manage full life-cycle development of the project. Involved in development of GUI for Client Using Java Swing Development using Java . , Java swing worker also developed the entire online help for the application system using the Java help system, the application had perspectives for views to support multiple tab Components in the UI, it was packaged as a web start build. Involved in creating ant scripts for the Jnlp files to update client builds for the Usability tests. Worked with Generics using Java . and some other open source tools to build the C C M Client like, Table Layout, Jakarta P etc, Worked with JDIC Java Desktop integration Components components to embed a web browser displaying HTML in a Java window. Worked on JMS to publish and subscribe to topics messages downstream to update legacy data using IBM MQ series. Created Configured Hibernate mapping classes, -Hibernate configuration files XML files to use the Hibernate frame work to update or view Database records, also created Criteria Queries to retrieve data from the Database. Developed RMT servers of client and server could communicate for data updates and retrievals. The combination of hibernate with RMI was simple and perfect solution for concurrency issues. Software Development Methodology used to develop the project was Agile Methodology with small scrum teams .Connected client to web service JAXRPC to retrieve data and d display for central storage, the application was developed over lie spring framework. Worked on writing updating ANT scripts while creating web archive files enterprise archive files the jars were signed to maintain security for the systems. Built high performance java servers and used maven for building packaging. Design of the Client side of the application using Java swing . using the JSR application framework. Collaborating with analysis team to review the developed UI and packaging deployment the application for UAT and SIT using java web start build techniques. Development tools JavaSwing,Intellijidea,YourKit Profiler Application server Websphere Server technology RMI Remote Method Invocation , Hibernate. Version Control Clearcase Special tools JDIC, Table Layout opensourceswings layout , Glazed Lists, lntelliJ IDEA, Your Kit Java Profiler to clear memory leaks in the client server code , Deadlocks P I Apache for exporting to excel from the GUI tables , hibernate.. Environment J EE . , JSP, JSTL, Ajax, Spring . , Struts . , Ajax, Hibernate . ,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log j, ORACLE g, Oracle Web logic Server . , SVN, Windows XP, UML, Sterling Commerce Distributed Order Management DOM . Specialty Planners Inc November - December Java J EE Developer Description Specialty Planners Inc. is an Insurance Agency sales company that sells Long Term Care LTC products of various Insurance companies called Carriers and gets commission for the business done, from the Carriers. Agents are recruited and trained by Specialty Planners to do business for the Carriers and the Carrier takes agent into contract. Specialty Planners supplies leads prospect customer information to its Agents via AIMS IC online application for doing business. The main objective of the AIMS application is that provides support and management for the Specialty Planners business. The system will allow Specialty Planners to operate efficiently and support brokerage business model and third party administration business. Responsibilities Involved in Unit Testing of the Application. Developed the JSPs and Modeling data using MVC architecture. Involved in designing front-end screens. Documenting daily weekly status report and sending to client Implemented the Servlets and JSP components Used Hibernate for Object Relational Mapping and data persistence. Developed the Database interaction classes using JDBC. Created JUnit test cases and ANT scripts for build automation. Environment Java, J EE . , HTML, XML, JDBC, JMS, Servlets, JSP . , Struts . , Hibernate, Web services, Eclipse . , Web Sphere , Oracle i, ANT, Microsoft Visio.",
  "preprocessed_text": "shelly wood professional summary year professional experience java jee including year hand experience big data hadoop ecosystem component expertise developing application financial domain using enterprise technology pertaining core java jee servlets jsp strut hibernate spring ioc spring mvc spring boot hibernate jms xml jdbc jndi jaxp jaxb web logic web sphere tomcat experience web service using xml html soap rest api solid background object oriented analysis design development implementation client server web enterprise development using n-tier architecture knowledge angular j practice creating custom general use module component extend element module core angular j strong experience creating real time data streaming solution using apache spark core spark sql data frame spark streaming depth knowledge hadoop architecture yarn experience writing map reduce program using apache hadoop analyzing big data hand experience writing ad-hoc query moving data hdfs hive analyzing data using hive ql experience importing exporting data using sqoop relational database system hdfs experience writing hadoop job analyzing data using pig latin working knowledge nosql database like hbase integrated apache kafka data ingestion experience using apache flume collecting aggregating moving large amount data application server experience using zookeeper oozie operational service coordinating cluster scheduling workflow extensive experience sql pl sql shell scripting database concept experience using version control management tool like cv svn rational clear case highly motivated self-starter positive attitude willingness learn new concept acceptance challenge ability work independently group peer results-driven environment strong analytical problem solving skill ability take initiative learn emerging technology programming language education m.c.a master computer application osmania university b.s.c computer science kakatiya university technical skill professional experience client usa april -persent title senior analytical hadoop spark developer ecomp critical achieving t imperative increase value network customer rapidly on-boarding new service created rd party enabling creation new ecosystem cloud consumer enterprise service reducing capital operational expenditure providing operation efficiency delivers enhanced customer experience allowing near real time reconfigure network service capacity ecomp directly support legacy physical element work traditional osss provide seamless customer experience across virtual physical element ecomp enables network agility elasticity improves time-to-market revenue scale via service design creation asdc visual modeling design service design creation capability policy recipe eliminate many manual long running process performed via traditional osss e.g. break-fix largely move plan build function ecomp platform provides external application os bs customer apps rd party integration secured restful api access control ecomp service responsibility developed spark script using scala shell command per requirement used spark api cloudera hadoop yarn perform analytics data hive developed scala script udffs using data frame sql rdd mapreduce spark data aggregation query writing data back oltp system sqoop experienced performance tuning spark application setting right batch interval time correct level parallelism memory tuning loaded data spark rdd memory data computation generate output response optimizing existing algorithm hadoop using spark context spark-sql data frame pair rdd s. involved loading data oracle database hdfs using sqoop query responsible building scalable distributed data solution using hadoop developed map reduce pipeline job process data create necessary hfiles developed pig latin script data cleansing environment hdfs hive hbase spark core spark sql spark streaming scala sbt oozie kafka- apache nifi java jdk unix svn zookeeper jee jsp jstl spring oracle maven rest-ful web service apache axis linux tomcat git jenikins client american express usa august -march title senior java hadoop developer project description american express company global travel financial network service provider company provides individual charge credit card ccsg build new application page given card charge credit enables prospect apply personal card prospect option apply personal card various channel long application user american express card holder want apply new card short application already card member option apply new card difference user asked fill minimal detail responsibility involved writing maven project primary responsibility include development web application using spring mvc hibernate pull query oracle involved loading data oracle database hdfs using sqoop query responsible building scalable distributed data solution using hadoop developed map reduce pipeline job process data create necessary hfiles developed pig latin script data cleansing worked different file format like textfile avrofile hive querying processing developed pig udf manipulating data according business requirement also worked developing custom pig loader successfully written spark streaming application read streaming twitter data analyze twitter record real time using yardstick framework measure performance apache ignite streaming apache spark streaming using apache nifi stream data feed kafka involved writing maven project primary responsibility include development web application using spring mvc hibernate pull query oracle implemented test case spark using scala language environment hadoop map reduce hdfs hive hbase spark core spark sql spark streaming scala sbt oozie kafka- apache nifi java jdk unix svn zookeeper jee jsp jstl spring oracle maven rest-ful web service soap apache axis linux tomcat client united overseas bank singapore september august title tech lead java hadoop description uob power lender ce loan origination system supporting kind loan main objective project build interface interact external fraud detection system secured unsecured loan system consists two basic module secured loan mortgage loan housing loan loan availed form bank providing security collateral unsecured loan credit card personal loan vehicle loan loan availed bank without security responsibility collaborated business intelligence team understand high level data roadmap define data discovery priority installed configured hadoop- map reduce hdfs developed multiple map reduce job java data cleaning preprocessing analyzed hadoop cluster using big data analytical tool pig hive scoop flume collected aggregated large amount web log data different source web server mobile network device using apache flume stored data hdfs analysis developed optimal strategy distributing web log data cluster importing exporting stored web log data hdfs hive using scoop involved creating hive table loading million record stored log data writing query invoke run map reduce job backend transformed large set semi-structured unstructured data various format extract parameter user location age spending time etc analyzed web log data using hive calculate metric number unique visitor view etc exported analyzed data relational database using sqoop visualization generating report designed efficient high-performing application extract transform load query large datasets including unstructured data installed apache oozie workflow engine run multiple hive pig job independently time data availability modelled user behavior based previous finding relevant data contributed development tool tracking understanding user behavior worked hive join produce input data set environment hadoop map reduce hdfs hive oracle hbase spark core spark sql spark streaming scala sbt oozie java jdk unix svn zookeeper java ee jsp jstl angularjs spring maven rest-ful web service soap apache axis linux tomcat maven universal weather aviation july august title senior java developer description application provide enhanced electronic version universal product uvtripplanner provide search facility ground service airport data web application provides information weather aviation detail registered user across globe user get data every single airport includes airport frequency custom service different type aircraft responsibility used agile methodology manage full life-cycle development project developed application using strut spring hibernate developed rich user interface using javascript jstl cs jquery jsps developed custom tag implementing logic jsps used java script jquery jstl cs strut tag developing jsps involved making release build deploying application test environment used oracle database backend database wrote sql update create database table used eclipse ide using ridc interface get content detail create content application used spring ioc injecting bean used hibernate connecting database mapping entity using hibernate annotation created junit test case unit testing application writing integration jsp communicating spring controller passing query criterion hibernate pull data showing report based search web gui streaming data excel involved writing maven project primary responsibility include development web application using spring mvc hibernate pull query oracle development tool maven spring mvc hibernate appserversiwebservers tomcat used junit jmock unit testing environment ee jsp jstl ajax spring strut ajax hibernate jdbc jndi xml xslt web service wsdl log oracle oracle web logic server svn window xp uml honeywell december july java developer description honeywell ap process cover processing vendor related payment client includes processing invoice credit note payment request etc overall basis timely accurate payment complete controllership diligent customer service required deliverable ap responsibility involved requirement gathering design development unit testing bug fixing used agile methodology manage full life-cycle development project involved development gui client using java swing development using java java swing worker also developed entire online help application system using java help system application perspective view support multiple tab component ui packaged web start build involved creating ant script jnlp file update client build usability test worked generic using java open source tool build client like table layout jakarta etc worked jdic java desktop integration component component embed web browser displaying html java window worked jms publish subscribe topic message downstream update legacy data using ibm mq series created configured hibernate mapping class -hibernate configuration file xml file use hibernate frame work update view database record also created criterion query retrieve data database developed rmt server client server could communicate data update retrieval combination hibernate rmi simple perfect solution concurrency issue software development methodology used develop project agile methodology small scrum team .connected client web service jaxrpc retrieve data display central storage application developed lie spring framework worked writing updating ant script creating web archive file enterprise archive file jar signed maintain security system built high performance java server used maven building packaging design client side application using java swing using jsr application framework collaborating analysis team review developed ui packaging deployment application uat sit using java web start build technique development tool javaswing intellijidea yourkit profiler application server websphere server technology rmi remote method invocation hibernate version control clearcase special tool jdic table layout opensourceswings layout glazed list lntellij idea kit java profiler clear memory leak client server code deadlock apache exporting excel gui table hibernate .. environment ee jsp jstl ajax spring strut ajax hibernate jdbc jndi xml xslt web service wsdl log oracle oracle web logic server svn window xp uml sterling commerce distributed order management dom specialty planner inc november december java ee developer description specialty planner inc. insurance agency sale company sell long term care ltc product various insurance company called carrier get commission business done carrier agent recruited trained specialty planner business carrier carrier take agent contract specialty planner supply lead prospect customer information agent via aim ic online application business main objective aim application provides support management specialty planner business system allow specialty planner operate efficiently support brokerage business model third party administration business responsibility involved unit testing application developed jsps modeling data using mvc architecture involved designing front-end screen documenting daily weekly status report sending client implemented servlets jsp component used hibernate object relational mapping data persistence developed database interaction class using jdbc created junit test case ant script build automation environment java ee html xml jdbc jms servlets jsp strut hibernate web service eclipse web sphere oracle ant microsoft visio",
  "statistics": {
    "word_count": 2836,
    "unique_word_count": 938,
    "avg_word_length": 4.86212976022567,
    "stopword_count": 607
  },
  "metadata": {
    "filename": "Shelly Woods.docx",
    "file_size": 42803,
    "processing_time": null,
    "processing_timestamp": "2025-09-09T23:21:00.696927",
    "input_filename": "Shelly Woods.json"
  }
}