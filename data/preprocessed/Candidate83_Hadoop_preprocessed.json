{
  "id": "Candidate83_Hadoop",
  "raw_text": "Candidate83\nSr. Hadoop Developer\nEmail: \tcandidate8308@gmail.com \t\t\t\t\t\t             Contact: (615) 813-1551\nPROFESSIONAL SUMMARY:\n8+ years of overall software development experience on Big Data Technologies, Hadoop Eco system and Java/J2EE Technologies with experience programming in Java, Scala, Python and SQL\n4+ years of strong hands-on experience on Hadoop Ecosystem including Spark, Map-Reduce, HIVE, Pig, HDFS, YARN, HBase, Oozie, Kafka, Sqoop, Flume.\nExperience in architecting, designing, and building distributed software systems. \nScala and Java, Created frameworks for processing data pipelines through Spark\nWrote python scripts to parse XML documents and load the data in database.\nDeep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance.\nUsed Sqoop to import data into HDFS / Hive from RDBMS and exporting data back to HDFS or HIVE from RDBMS.\nWorked with real-time data processing and streaming techniques using Spark streaming, Storm and Kafka.\nExperience developing Kafka producers and Kafka Consumers for streaming millions of events per second on streaming data\nSignificant experience writing custom UDF’s in Hive and custom Input Formats in MapReduce.\nKnowledge of job workflow scheduling and monitoring tools like Oozie.\nStrong experience productionalizing end to end data pipelines on hadoop platform.\nExpertise in Database Design, Creation and Management of Schemas, writing Stored Procedures, Functions, DDL and DML SQL queries and writing complex queries for Oracle\nExperience working with NoSQL database technologies, including MongoDB, Cassandra and HBase.\nGood experience is designing and implementing end to end Data Security and Governance within Hadoop Platform using Kerberos.\nStrong experience with UNIX shell scripts and commands.\nExperience in using various Hadoop Distributions like Cloudera, Hortonworks and Amazon EMR.\nStrong hands-on development experience with Java, J2EE (Servlets, JSP, Java Beans, EJB, JDBC, JMS, Web Services) and related technologies.\nWork with the team to help understand requirements, evaluate new features, architecture and help drive decisions.\nExcellent interpersonal, communication, problem solving and analytical skills with ability to make independent decisions\nExperience successfully delivering applications using agile methodologies including extreme programming, SCRUM and Test-Driven Development (TDD).\nExperience in Object Oriented Analysis, Design, and Programming of distributed web-based applications.\nExtensive experience in developing standalone multithreaded applications.\nConfigured and developed web applications in Spring and employed spring MVC architecture and Inversion of Control.\nExperience in building, deploying and integrating applications in Application Servers with ANT, Maven and Gradle.\nSignificant application development experience with REST Web Services, SOAP, WSDL, and XML.\nTECHNICAL SKILLS:\nPROFESSIONAL EXPERIENCE:\nClient\t\t: \tTMNAS\t\t\t\t\t\t        Sep 2016 – Present\nLocation\t: \tBala Cynwyd, PA\t\nRole\t\t:\tSr. Hadoop Developer\nProject Description: TMNA offers the security of nearby expertise, enhanced by the diversity and power of one of the world’s most respected insurance groups. Tokio Marine’s companies offer access to leading commercial insurance solutions spanning the property and casualty landscape including professional liability, workers’ compensation and property coverage.. The project deals with analyzing clickstream data of users who are visiting the company websites and applications to derive useful insights that help in optimizing future promotions and advertising.\nResponsibilities:\nInvolved in story-driven agile development methodology and actively participated in daily scrum meetings.\nIngested terabytes of click stream data from external systems like FTP Servers and S3 buckets into HDFS using custom Input Adaptors.\nImplemented end-to-end pipelines for performing user behavioral analytics to identify user-browsing patterns and provide rich experience and personalization to the visitors.\nDeveloped Kafka producers for streaming real-time clickstream events from external Rest services into topics.\nUsed HDFS File System API to connect to FTP Server and HDFS.  S3 AWS SDK for connecting to S3 buckets.\nWritten Scala based Spark applications for performing various data transformations, denormalization, and other custom processing.\nImplemented data pipeline using Spark, Hive, Sqoop and Kafka to ingest customer behavioral data into Hadoop platform to perform user behavioral analytics.\nCreated a multi-threaded Java application running on edge node for pulling the raw clickstream data from FTP servers and AWS S3 buckets.\nDeveloped Spark streaming jobs using Scala for real time processing.\nInvolved in creating external Hive tables from the files stored in the HDFS.\nOptimized the Hive tables utilizing improvement techniques like partitions and bucketing to give better execution Hive QL queries.\nUsed Spark-SQL to read data from hive tables, and perform various transformations like changing date format and breaking complex columns. \nWrote spark application to load the transformed data back into the Hive tables using parquet format.\nUsed Oozie Scheduler system to automate the pipeline workflow to exact data on a timely manner. \nImplemented installation and configuration of multi-node cluster on the cloud using Amazon \nWeb Services (AWS) on EC2.\nWorked on data visualization and analytics with research scientist and business stake holders.\nEnvironment: Hadoop 2.x, Spark, Scala, Hive, Pig, Sqoop, Oozie, Kafka, Cloudera Manager, Storm, ZooKeeper, HBase, Impala, YARN, Cassandra, JIRA, MySQL, Kerberos, Amazon AWS, Shell Scripting, SBT, Git, Maven.\nClient\t\t:\tDavita Inc\t\t\t    \t\t\t    Jan 2015 - Sep 2016 \nLocation\t:\tNashville, Tennessee\nRole\t\t:\tSr.Hadoop Developer\nProject Description: Davita is one of the largest kidney dialysis companies in world. The idea of the project is to ingest data from different multiple sources to Hadoop Data Lake, perform transformations on it according to business requirements and exporting the data to external systems. The system is a scalable BI platform that can adapt to the speed of the business by providing relevant, accessible, timely, connected, and accurate data.\nResponsibilities:\nInvolved in gathering and analyzing business requirements and designing Data Lake as per the requirements.\nBuilt distributed, scalable, and reliable data pipelines that ingest and process data at scale using Hive and MapReduce.\nDeveloped MapReduce jobs in Java for cleansing the data and preprocessing.\nLoaded transactional data from Teradata using Sqoop and create Hive Tables.\nExtensively used Sqoop for efficiently transferring bulk data between HDFS and relational databases.\nWorked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition, Bucketing in hive and Map Side joins.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nUsed IMPALA to analyze the data present in Hive tables.\nHandled Avro and JSON data in Hive using Hive SerDe.\nWorked with different compression codecs like GZIP, SNAPPY and BZIP2 in MapReduce, Pig and Hive for better performance.\nAnalyzed the data by performing the Hive queries using Hive QL to study the customer behavior.\nWrote python scripts to parse XML documents and load the data in database.\nGenerate auto mails by using Python scripts.\nImplemented the recurring workflows using Oozie to automate the scheduling flow.\nWorked with application teams to install OS level updates and version upgrades for Hadoop cluster environments.\nParticipated in design and code reviews.\nEnvironment: HDFS, Hadoop, Pig, Hive, HBase, Sqoop, Talend, Flume, Map Reduce, Podium Data, Oozie, Java 6/7, Oracle 10g, YARN, UNIX Shell Scripting, SOAP, REST services, Oracle 10g, Maven, Agile Methodology, JIRA.\nClient\t\t:\tNASBA\t\t\t\t\t\t\t    Aug 2012 - Dec 2014 \nLocation\t:\tNashville, TN\nRole\t\t:\tHadoop Developer\nProject Description: National Association of State Boards of Accountancy enhances the effectiveness and advance the common interests of the Boards of Accountancy. Existing ETL platform is overloaded with data coming from variety of sources and as data is growing day by day, it is not able to perform well and cost of managing the Relational database servers are going up. So, the data is migrated from multiple sources to Hadoop Data Lake and transformations are performed on it according to business requirements and the processed data is exported to external systems.\nResponsibilities: \nAnalysed business requirements and created/updated Software Requirements and design documents\t\nImported the data from relational databases to Hadoop cluster by using Sqoop. \nProvided batch processing solution to certain unstructured and large volume of data by using Hadoop Map Reduce framework.\nDeveloped data pipelines using Hive scripts to transform data from Teradata, DB2 data sources. These pipelines had customized UDF'S to extend the ETL functionality. \nDeveloped UDF for converting data from Hive table to JSON format as per client requirement. \nInvolved in creating tables in Hive and writing scripts and queries to load data into Hive tables from HDFS.\nImplemented dynamic partitioning and Bucketing in Hive as part of performance tuning. \nCreated custom UDF’s in Pig and Hive.\nPerformed various transformations on data like changing date patterns, converting to other time zones etc.\nDesigned and developed PIG Latin Scripts to process data in a batch to perform trend analysis. \nAutomated Sqoop, hive and pig jobs using Oozie scheduling. \nStoring, processing and analyzing huge data-set for getting valuable insights from them. \nCreated various aggregated datasets for easy and faster reporting using Tableau.\nEnvironment: HDFS, Map Reduce, Hive, Sqoop, Pig, HBase, Oozie, CDH distribution, Java, Eclipse, Shell Scripts, Tableau, Windows, Linux.\nClient\t\t:\tCopart Inc\t\t\t\t\t\t    Oct 2010 - Aug 2012 \nLocation\t:\tDallas, TX\nRole\t\t:\tJava Developer\nProject Description:  Copart makes it easy for Members to find, bid and win the vehicles that they are looking for. Members can choose from , early and late model  and ,  and more. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\nResponsibilities:\nDeveloped the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates.\nWorked in all the modules of the application which involved front-end presentation logic - developed using Spring MVC, JSP, JSTL and JavaScript, Business objects - developed using POJOs and data access layer - using Hibernate framework. \nDesigned the GUI of the application using JavaScript, HTML, CSS, Servlets, and JSP.\nInvolved in writing AJAX scripts for the requests to process quickly.\nUsed Dependency Injection feature and AOP features of Spring framework to handle exceptions.\nInvolved in writing Hibernate Query Language (HQL) for persistence layer.\nImplemented persistence layer using Hibernate that uses the POJOs to represent the persistence database.\nUsed JDBC to connect to backend databases, Oracle and SQL Server 2005.  \nProficient in writing SQL queries, stored procedures for multiple databases, Oracle and SQL Server.\nWrote backend jobs based on Core Java & Oracle Data Base to be run daily/weekly.\nUsed Restful API and SOAP web services for internal and external consumption.\nUsed Core Java concepts like Collections, Garbage Collection, Multithreading, OOPs concepts and APIs to do encryption and compression of incoming request to provide security.\nWritten and implemented test scripts to support Test driven development (TDD) and continuous integration.\nEnvironment: Java, JSP, HTML, CSS, Ubuntu Operating System, JavaScript, AJAX, Servlets, Struts, Hibernate, EJB (Session Beans), Log4J, WebSphere, JNDI, Oracle, Windows XP, LINUX, ANT, Eclipse.\nClient\t\t:\tAricent\t\t\t\t\t\t\t    Nov 2008 - Sep 2010    \nLocation\t:\tNewyork\nRole\t\t:\tJava Developer\nProject Description:  Aricent is a global design and engineering company innovating in the digital era. help the world's leading companies solve their most important business and technology innovation challenges - from Customer to Chip. I have worked on developing the Aricent internal applications to automate the business process, store the documents. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\nResponsibilities:\nAnalyzed user requirements and created Software Requirements and design documents\nResponsible for GUI development using Java, JSP, Struts\nDatabase design and development \nCreated and modified existing database scripts, Tables, Stored Procedures, and Triggers \nUsed XML functions, Cursors, Mail and Utility packages for Advanced search functionality\nCreated data correction and manipulation scripts for Production\nUsed JAXB for marshalling and un-marshalling of the data\nCreated JUnit tests for the service layer\nSupport for Production issues\nAttending the review meetings for scheduling, implementation and resolving issues in software development cycle\nEnvironment: Java, Struts, Java, Jsp, Servlets, JQuery, Ajax, XML, XSLT, JAXB, FOP, JBoss, Weblogic, Tomcat, SQL server 2005 and MyEclipse\nEDUCATION:\nBachelor of Technology in Computer Science.               ",
  "cleaned_text": "Candidate Sr. Hadoop Developer Email Contact PROFESSIONAL SUMMARY years of overall software development experience on Big Data Technologies, Hadoop Eco system and Java J EE Technologies with experience programming in Java, Scala, Python and SQL years of strong hands-on experience on Hadoop Ecosystem including Spark, Map-Reduce, HIVE, Pig, HDFS, YARN, HBase, Oozie, Kafka, Sqoop, Flume. Experience in architecting, designing, and building distributed software systems. Scala and Java, Created frameworks for processing data pipelines through Spark Wrote python scripts to parse XML documents and load the data in database. Deep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance. Used Sqoop to import data into HDFS Hive from RDBMS and exporting data back to HDFS or HIVE from RDBMS. Worked with real-time data processing and streaming techniques using Spark streaming, Storm and Kafka. Experience developing Kafka producers and Kafka Consumers for streaming millions of events per second on streaming data Significant experience writing custom UDFs in Hive and custom Input Formats in MapReduce. Knowledge of job workflow scheduling and monitoring tools like Oozie. Strong experience productionalizing end to end data pipelines on hadoop platform. Expertise in Database Design, Creation and Management of Schemas, writing Stored Procedures, Functions, DDL and DML SQL queries and writing complex queries for Oracle Experience working with NoSQL database technologies, including MongoDB, Cassandra and HBase. Good experience is designing and implementing end to end Data Security and Governance within Hadoop Platform using Kerberos. Strong experience with UNIX shell scripts and commands. Experience in using various Hadoop Distributions like Cloudera, Hortonworks and Amazon EMR. Strong hands-on development experience with Java, J EE Servlets, JSP, Java Beans, EJB, JDBC, JMS, Web Services and related technologies. Work with the team to help understand requirements, evaluate new features, architecture and help drive decisions. Excellent interpersonal, communication, problem solving and analytical skills with ability to make independent decisions Experience successfully delivering applications using agile methodologies including extreme programming, SCRUM and Test-Driven Development TDD . Experience in Object Oriented Analysis, Design, and Programming of distributed web-based applications. Extensive experience in developing standalone multithreaded applications. Configured and developed web applications in Spring and employed spring MVC architecture and Inversion of Control. Experience in building, deploying and integrating applications in Application Servers with ANT, Maven and Gradle. Significant application development experience with REST Web Services, SOAP, WSDL, and XML. TECHNICAL SKILLS PROFESSIONAL EXPERIENCE Client TMNAS Sep Present Location Bala Cynwyd, PA Role Sr. Hadoop Developer Project Description TMNA offers the security of nearby expertise, enhanced by the diversity and power of one of the worlds most respected insurance groups. Tokio Marines companies offer access to leading commercial insurance solutions spanning the property and casualty landscape including professional liability, workers compensation and property coverage.. The project deals with analyzing clickstream data of users who are visiting the company websites and applications to derive useful insights that help in optimizing future promotions and advertising. Responsibilities Involved in story-driven agile development methodology and actively participated in daily scrum meetings. Ingested terabytes of click stream data from external systems like FTP Servers and S buckets into HDFS using custom Input Adaptors. Implemented end-to-end pipelines for performing user behavioral analytics to identify user-browsing patterns and provide rich experience and personalization to the visitors. Developed Kafka producers for streaming real-time clickstream events from external Rest services into topics. Used HDFS File System API to connect to FTP Server and HDFS. S AWS SDK for connecting to S buckets. Written Scala based Spark applications for performing various data transformations, denormalization, and other custom processing. Implemented data pipeline using Spark, Hive, Sqoop and Kafka to ingest customer behavioral data into Hadoop platform to perform user behavioral analytics. Created a multi-threaded Java application running on edge node for pulling the raw clickstream data from FTP servers and AWS S buckets. Developed Spark streaming jobs using Scala for real time processing. Involved in creating external Hive tables from the files stored in the HDFS. Optimized the Hive tables utilizing improvement techniques like partitions and bucketing to give better execution Hive QL queries. Used Spark-SQL to read data from hive tables, and perform various transformations like changing date format and breaking complex columns. Wrote spark application to load the transformed data back into the Hive tables using parquet format. Used Oozie Scheduler system to automate the pipeline workflow to exact data on a timely manner. Implemented installation and configuration of multi-node cluster on the cloud using Amazon Web Services AWS on EC . Worked on data visualization and analytics with research scientist and business stake holders. Environment Hadoop .x, Spark, Scala, Hive, Pig, Sqoop, Oozie, Kafka, Cloudera Manager, Storm, ZooKeeper, HBase, Impala, YARN, Cassandra, JIRA, MySQL, Kerberos, Amazon AWS, Shell Scripting, SBT, Git, Maven. Client Davita Inc Jan - Sep Location Nashville, Tennessee Role Sr.Hadoop Developer Project Description Davita is one of the largest kidney dialysis companies in world. The idea of the project is to ingest data from different multiple sources to Hadoop Data Lake, perform transformations on it according to business requirements and exporting the data to external systems. The system is a scalable BI platform that can adapt to the speed of the business by providing relevant, accessible, timely, connected, and accurate data. Responsibilities Involved in gathering and analyzing business requirements and designing Data Lake as per the requirements. Built distributed, scalable, and reliable data pipelines that ingest and process data at scale using Hive and MapReduce. Developed MapReduce jobs in Java for cleansing the data and preprocessing. Loaded transactional data from Teradata using Sqoop and create Hive Tables. Extensively used Sqoop for efficiently transferring bulk data between HDFS and relational databases. Worked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive. Worked on various performance optimizations like using distributed cache for small datasets, Partition, Bucketing in hive and Map Side joins. Created components like Hive UDFs for missing functionality in HIVE for analytics. Used IMPALA to analyze the data present in Hive tables. Handled Avro and JSON data in Hive using Hive SerDe. Worked with different compression codecs like GZIP, SNAPPY and BZIP in MapReduce, Pig and Hive for better performance. Analyzed the data by performing the Hive queries using Hive QL to study the customer behavior. Wrote python scripts to parse XML documents and load the data in database. Generate auto mails by using Python scripts. Implemented the recurring workflows using Oozie to automate the scheduling flow. Worked with application teams to install OS level updates and version upgrades for Hadoop cluster environments. Participated in design and code reviews. Environment HDFS, Hadoop, Pig, Hive, HBase, Sqoop, Talend, Flume, Map Reduce, Podium Data, Oozie, Java , Oracle g, YARN, UNIX Shell Scripting, SOAP, REST services, Oracle g, Maven, Agile Methodology, JIRA. Client NASBA Aug - Dec Location Nashville, TN Role Hadoop Developer Project Description National Association of State Boards of Accountancy enhances the effectiveness and advance the common interests of the Boards of Accountancy. Existing ETL platform is overloaded with data coming from variety of sources and as data is growing day by day, it is not able to perform well and cost of managing the Relational database servers are going up. So, the data is migrated from multiple sources to Hadoop Data Lake and transformations are performed on it according to business requirements and the processed data is exported to external systems. Responsibilities Analysed business requirements and created updated Software Requirements and design documents Imported the data from relational databases to Hadoop cluster by using Sqoop. Provided batch processing solution to certain unstructured and large volume of data by using Hadoop Map Reduce framework. Developed data pipelines using Hive scripts to transform data from Teradata, DB data sources. These pipelines had customized UDF S to extend the ETL functionality. Developed UDF for converting data from Hive table to JSON format as per client requirement. Involved in creating tables in Hive and writing scripts and queries to load data into Hive tables from HDFS. Implemented dynamic partitioning and Bucketing in Hive as part of performance tuning. Created custom UDFs in Pig and Hive. Performed various transformations on data like changing date patterns, converting to other time zones etc. Designed and developed PIG Latin Scripts to process data in a batch to perform trend analysis. Automated Sqoop, hive and pig jobs using Oozie scheduling. Storing, processing and analyzing huge data-set for getting valuable insights from them. Created various aggregated datasets for easy and faster reporting using Tableau. Environment HDFS, Map Reduce, Hive, Sqoop, Pig, HBase, Oozie, CDH distribution, Java, Eclipse, Shell Scripts, Tableau, Windows, Linux. Client Copart Inc Oct - Aug Location Dallas, TX Role Java Developer Project Description Copart makes it easy for Members to find, bid and win the vehicles that they are looking for. Members can choose from , early and late model and , and more. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents. Responsibilities Developed the J EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates. Worked in all the modules of the application which involved front-end presentation logic - developed using Spring MVC, JSP, JSTL and JavaScript, Business objects - developed using POJOs and data access layer - using Hibernate framework. Designed the GUI of the application using JavaScript, HTML, CSS, Servlets, and JSP. Involved in writing AJAX scripts for the requests to process quickly. Used Dependency Injection feature and AOP features of Spring framework to handle exceptions. Involved in writing Hibernate Query Language HQL for persistence layer. Implemented persistence layer using Hibernate that uses the POJOs to represent the persistence database. Used JDBC to connect to backend databases, Oracle and SQL Server . Proficient in writing SQL queries, stored procedures for multiple databases, Oracle and SQL Server. Wrote backend jobs based on Core Java & Oracle Data Base to be run daily weekly. Used Restful API and SOAP web services for internal and external consumption. Used Core Java concepts like Collections, Garbage Collection, Multithreading, OOPs concepts and APIs to do encryption and compression of incoming request to provide security. Written and implemented test scripts to support Test driven development TDD and continuous integration. Environment Java, JSP, HTML, CSS, Ubuntu Operating System, JavaScript, AJAX, Servlets, Struts, Hibernate, EJB Session Beans , Log J, WebSphere, JNDI, Oracle, Windows XP, LINUX, ANT, Eclipse. Client Aricent Nov - Sep Location Newyork Role Java Developer Project Description Aricent is a global design and engineering company innovating in the digital era. help the world s leading companies solve their most important business and technology innovation challenges - from Customer to Chip. I have worked on developing the Aricent internal applications to automate the business process, store the documents. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents. Responsibilities Analyzed user requirements and created Software Requirements and design documents Responsible for GUI development using Java, JSP, Struts Database design and development Created and modified existing database scripts, Tables, Stored Procedures, and Triggers Used XML functions, Cursors, Mail and Utility packages for Advanced search functionality Created data correction and manipulation scripts for Production Used JAXB for marshalling and un-marshalling of the data Created JUnit tests for the service layer Support for Production issues Attending the review meetings for scheduling, implementation and resolving issues in software development cycle Environment Java, Struts, Java, Jsp, Servlets, JQuery, Ajax, XML, XSLT, JAXB, FOP, JBoss, Weblogic, Tomcat, SQL server and MyEclipse EDUCATION Bachelor of Technology in Computer Science.",
  "preprocessed_text": "candidate sr. hadoop developer contact professional summary year overall software development experience big data technology hadoop eco system java ee technology experience programming java scala python sql year strong hands-on experience hadoop ecosystem including spark map-reduce hive pig hdfs yarn hbase oozie kafka sqoop flume experience architecting designing building distributed software system scala java created framework processing data pipeline spark wrote python script parse xml document load data database deep knowledge troubleshooting tuning spark application hive script achieve optimal performance used sqoop import data hdfs hive rdbms exporting data back hdfs hive rdbms worked real-time data processing streaming technique using spark streaming storm kafka experience developing kafka producer kafka consumer streaming million event per second streaming data significant experience writing custom udfs hive custom input format mapreduce knowledge job workflow scheduling monitoring tool like oozie strong experience productionalizing end end data pipeline hadoop platform expertise database design creation management schema writing stored procedure function ddl dml sql query writing complex query oracle experience working nosql database technology including mongodb cassandra hbase good experience designing implementing end end data security governance within hadoop platform using kerberos strong experience unix shell script command experience using various hadoop distribution like cloudera hortonworks amazon emr strong hands-on development experience java ee servlets jsp java bean ejb jdbc jms web service related technology work team help understand requirement evaluate new feature architecture help drive decision excellent interpersonal communication problem solving analytical skill ability make independent decision experience successfully delivering application using agile methodology including extreme programming scrum test-driven development tdd experience object oriented analysis design programming distributed web-based application extensive experience developing standalone multithreaded application configured developed web application spring employed spring mvc architecture inversion control experience building deploying integrating application application server ant maven gradle significant application development experience rest web service soap wsdl xml technical skill professional experience client tmnas sep present location bala cynwyd pa role sr. hadoop developer project description tmna offer security nearby expertise enhanced diversity power one world respected insurance group tokio marine company offer access leading commercial insurance solution spanning property casualty landscape including professional liability worker compensation property coverage .. project deal analyzing clickstream data user visiting company website application derive useful insight help optimizing future promotion advertising responsibility involved story-driven agile development methodology actively participated daily scrum meeting ingested terabyte click stream data external system like ftp server bucket hdfs using custom input adaptor implemented end-to-end pipeline performing user behavioral analytics identify user-browsing pattern provide rich experience personalization visitor developed kafka producer streaming real-time clickstream event external rest service topic used hdfs file system api connect ftp server hdfs aws sdk connecting bucket written scala based spark application performing various data transformation denormalization custom processing implemented data pipeline using spark hive sqoop kafka ingest customer behavioral data hadoop platform perform user behavioral analytics created multi-threaded java application running edge node pulling raw clickstream data ftp server aws bucket developed spark streaming job using scala real time processing involved creating external hive table file stored hdfs optimized hive table utilizing improvement technique like partition bucketing give better execution hive ql query used spark-sql read data hive table perform various transformation like changing format breaking complex column wrote spark application load transformed data back hive table using parquet format used oozie scheduler system automate pipeline workflow exact data timely manner implemented installation configuration multi-node cluster cloud using amazon web service aws ec worked data visualization analytics research scientist business stake holder environment hadoop .x spark scala hive pig sqoop oozie kafka cloudera manager storm zookeeper hbase impala yarn cassandra jira mysql kerberos amazon aws shell scripting sbt git maven client davita inc jan sep location nashville tennessee role sr.hadoop developer project description davita one largest kidney dialysis company world idea project ingest data different multiple source hadoop data lake perform transformation according business requirement exporting data external system system scalable bi platform adapt speed business providing relevant accessible timely connected accurate data responsibility involved gathering analyzing business requirement designing data lake per requirement built distributed scalable reliable data pipeline ingest process data scale using hive mapreduce developed mapreduce job java cleansing data preprocessing loaded transactional data teradata using sqoop create hive table extensively used sqoop efficiently transferring bulk data hdfs relational database worked automation delta feed teradata using sqoop ftp server hive worked various performance optimization like using distributed cache small datasets partition bucketing hive map side join created component like hive udfs missing functionality hive analytics used impala analyze data present hive table handled avro json data hive using hive serde worked different compression codecs like gzip snappy bzip mapreduce pig hive better performance analyzed data performing hive query using hive ql study customer behavior wrote python script parse xml document load data database generate auto mail using python script implemented recurring workflow using oozie automate scheduling flow worked application team install o level update version upgrade hadoop cluster environment participated design code review environment hdfs hadoop pig hive hbase sqoop talend flume map reduce podium data oozie java oracle yarn unix shell scripting soap rest service oracle maven agile methodology jira client nasba aug dec location nashville tn role hadoop developer project description national association state board accountancy enhances effectiveness advance common interest board accountancy existing etl platform overloaded data coming variety source data growing day day able perform well cost managing relational database server going data migrated multiple source hadoop data lake transformation performed according business requirement processed data exported external system responsibility analysed business requirement created updated software requirement design document imported data relational database hadoop cluster using sqoop provided batch processing solution certain unstructured large volume data using hadoop map reduce framework developed data pipeline using hive script transform data teradata db data source pipeline customized udf extend etl functionality developed udf converting data hive table json format per client requirement involved creating table hive writing script query load data hive table hdfs implemented dynamic partitioning bucketing hive part performance tuning created custom udfs pig hive performed various transformation data like changing pattern converting time zone etc designed developed pig latin script process data batch perform trend analysis automated sqoop hive pig job using oozie scheduling storing processing analyzing huge data-set getting valuable insight created various aggregated datasets easy faster reporting using tableau environment hdfs map reduce hive sqoop pig hbase oozie cdh distribution java eclipse shell script tableau window linux client copart inc oct aug location dallas tx role java developer project description copart make easy member find bid win vehicle looking member choose early late model internal application used content management user new site used store project related document responsibility developed ee application based service oriented architecture employing soap tool data exchange update worked module application involved front-end presentation logic developed using spring mvc jsp jstl javascript business object developed using pojos data access layer using hibernate framework designed gui application using javascript html cs servlets jsp involved writing ajax script request process quickly used dependency injection feature aop feature spring framework handle exception involved writing hibernate query language hql persistence layer implemented persistence layer using hibernate us pojos represent persistence database used jdbc connect backend database oracle sql server proficient writing sql query stored procedure multiple database oracle sql server wrote backend job based core java oracle data base run daily weekly used restful api soap web service internal external consumption used core java concept like collection garbage collection multithreading oops concept apis encryption compression incoming provide security written implemented test script support test driven development tdd continuous integration environment java jsp html cs ubuntu operating system javascript ajax servlets strut hibernate ejb session bean log websphere jndi oracle window xp linux ant eclipse client aricent nov sep location newyork role java developer project description aricent global design engineering company innovating digital era help world leading company solve important business technology innovation challenge customer chip worked developing aricent internal application automate business process store document internal application used content management user new site used store project related document responsibility analyzed user requirement created software requirement design document responsible gui development using java jsp strut database design development created modified existing database script table stored procedure trigger used xml function cursor mail utility package advanced search functionality created data correction manipulation script production used jaxb marshalling un-marshalling data created junit test service layer support production issue attending review meeting scheduling implementation resolving issue software development cycle environment java strut java jsp servlets jquery ajax xml xslt jaxb fop jboss weblogic tomcat sql server myeclipse education bachelor technology computer science",
  "statistics": {
    "word_count": 2213,
    "unique_word_count": 754,
    "avg_word_length": 5.133303208314505,
    "stopword_count": 494
  },
  "metadata": {
    "filename": "Candidate83_Hadoop.docx",
    "file_size": 34833,
    "processing_time": null,
    "processing_timestamp": "2025-09-09T23:20:57.959840",
    "input_filename": "Candidate83_Hadoop.json"
  }
}