{
  "id": "Candidate23 Hadoop developer",
  "raw_text": "Candidate23\nSr. Hadoop Developer\nPhone: +1(224)-706-0020   \nEmail: \n------------------------------------------------------------------------------------------------------------------------------------------\nPROFESSIONAL SUMMARY:\nOver 8+ years of experience including 4 years of Big Data Ecosystem related technologies with full project development, implementation and deployment.\nStrong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie.\nStrong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. \nIn-depth understanding of MapReduce Framework and Spark execution model.\nWorked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching.\nStrong experience working with both batch and real-time processing using Spark framework.\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nHands on experience in installing, configuring and deploying Hadoop distributions in cloud environments (Amazon Web Services).\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nWorked on building real time data workflows using Kafka, Spark streaming and HBase. \nWorked extensively on Hive for building complex data analytical applications.\nVery good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.\nUsed custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. \nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExperienced in Cluster coordination services through zookeeper.\nStrong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats.\nWorked with Sqoop to move (import/export) data from a relational database into Hadoop.\nExperience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions.\nExtensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts.\nDesigned and implemented Hive and Pig UDF's using Java for evaluation, filtering, loading and storing of data.\nExperienced in job workflow scheduling and monitoring tools like Oozie.\nWell versed with UNIX and Linux command line and shell script.\nAdequate knowledge and working experience with agile methodology.\nTECHNICAL SKILLS:\nEDUCATION:\nBachelor of Technology in Computer Science Engineering \nWORK EXPERIENCE:\nCigna – Bloomfield, Connecticut                                                             \t\t       Jul’17 – Present \nRole: Hadoop/Spark Developer \nResponsibilities:\nDeveloped Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data.\nDeveloped highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement\nData pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data.\nDeveloped Spark jobs and Hive Jobs to summarize and transform data.\nUsed Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns.\nInvolved in converting Hive queries into Spark transformations using Spark Data Frames in Scala.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nBuilt real time data pipelines by developing Kafka producers and spark streaming applications for consuming.\nIngested syslog messages to Kafka.\nWorked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals.\nHandled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark.\nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team.\nExperienced in cluster coordination services through Zookeeper.\nInstalled, tested and deployed monitoring solutions with Splunk services.\nUsed Hive to analyze the partitioned and bucketed data and computed various metrics for reporting.\nDeveloped Hive scripts in Hive QL to de-normalize and aggregate the data.\nScheduled and executed workflows in Oozie to run various jobs.\nDesigning & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases.\nEnvironment: Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase.\nQualcomm -- San Diego, CA                                                                   \t\t          Dec’16 – Jun’17                                                                                            \nRole: Hadoop/Spark Developer\nResponsibilities:\nPart of Big Data Center of Excellence (CoE), responsible for designing and building enterprise data analytics platform.\nWorked with respective business units in understanding the scope of the analytics requirements.\nPerformed core ETL transformations in Spark.\nAutomated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics.\nCreated end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data.\nDeveloped end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala.\nImplemented Spark utilizing Spark-SQL heavily for faster development, and processing of data.  \nExploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode.\nHandled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables.\nCollecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis\nWrapper developed in Python for instantiating multithreaded application and running with other applications.\nAnalyzed the data by performing Hive queries (Hive QL) and running Pig scripts (Pig Latin) to study customer behavior.\nData warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse.\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition, Bucketing in Hive and Map Side joins.\nCreated Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nEnvironment: AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper.\nHortonworks\nMcKesson - Alpharetta, GA                                                                   \t\t        Dec’15 – Nov’16\nHadoop/Spark Developer\nResponsibilities:\nDeveloped multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S3 buckets on daily basis.\nCreated various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users.\nImplemented batch processing of jobs using Spark Scala API.\nDeveloped Sqoop scripts to import/export data from Oracle to HDFS and into Hive tables. \nStored the data in columnar formats using Hive.\nInvolved building and managing NoSQL Database models using HBase.\nWorked in Spark to read the data from Hive and write it to Hbase.\nOptimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries. \nWorked with multiple file formats like Avro, Sequence, Parquet and Orc.\nConverted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data.\nLoaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications.\nWorked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame.\nImplemented business logic in Hive and written UDF’s to process the data for analysis.\nUsed Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs.\nAddressing the issues occurring due to the huge volume of data and transitions.\nDesigned, documented operational problems by following standards and procedures using JIRA. \nEnvironment: Java, Hadoop 2.1.0, Map Reduce2, Spark, Unix, Pig 0.12.0, Hive 0.13.0, Linux, Sqoop 1.4.2, Flume 1.3.1, Eclipse, AWS EC2, and Cloudera CDH 4.  \nAmerican Home Shield - Memphis, TN                                                \t\t         Dec’14 – Nov’15\nRole: Hadoop Developer\nResponsibilities:\nMigrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS.\nMainly worked on Hive queries to categorize data of different claims.\nInvolved in loading data from LINUX file system to HDFS\nWritten customized Hive UDFs in Java where the functionality is too complex.\nImplemented Partitioning, Dynamic Partitions, Buckets in HIVE.\nDesigning and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.\nGenerate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector.\nResponsible to manage the test data coming from different sources\nReviewing peer table creation in Hive, data loading and queries.\nWeekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.\nMonitored System health and logs and respond accordingly to any warning or failure conditions.\nGained experience in managing and reviewing Hadoop log files.\nInvolved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\nInvolved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\nEnvironment: Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH4, Oracle, MySQL.\nProtective Life - Edina, MN                                                                      \t\t         Oct’13 - Nov’14     \nRole: Java Developer\nResponsibilities:\nImplemented a Web based Application using Servlets, JSP, spring, JDBC, XML. \nInvolved in writing Spring Configuration XML file that contains declarations and other   dependent objects declarations. \nUsed hibernate to connect to Database to create the DAO layer. \nDeveloped Application Framework using Model-View-Controller using the technology Spring. \nUsed HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages \nMultilayer Applications construction using Open JPA, HTML5, Spring MVC. \nAnnotated Spring Architecture (Spring Beans) \nImplemented UNIX shell scripts to migrate various data files to S&P ratings repository \nImplemented smooth pagination capability using JSP to remove existing pagination utility \nWorked on Geo API to provide geological access capability to S&P.com site. \nInvolved in Agile process to streamline development process with iterative development. \nCode reviews and Managing the CVS Repository. \nPrepare builds for DEV and UAT environments. \nParticipating in the regular team meetings sprint planning meetings, user story review meetings etc. \nInvolved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool. \nEnvironment: JDK 1.5, XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log4J, CSS Style Sheets, Apache Tomcat, J2EE, Maven 3\nAccenture – USA\t\t       Oct’11– Sep’13\nRole: Java Developer\nResponsibilities:\nInvolved in Requirements analysis, design, and development and testing.\nInvolved in setting up the different roles & maintained authentication to the application.\nDesigned, deployed and tested Multi-tier application using the Java technologies.\nInvolved in front end development using JSP, HTML & CSS.\nImplemented the Application using Servlets\nDeployed the application on Oracle Web logic server\nImplemented Multithreading concepts in java classes to avoid deadlocking.\nUsed MySQL database to store data and execute SQL queries on the backend.\nPrepared and Maintained test environment.\nTested the application before going live to production.\nDocumented and communicated test result to the team lead on daily basis.\nInvolved in weekly meeting with team leads and manager to discuss the issues and status of the projects.\nEnvironment: J2EE (Java, JSP, JDBC, Multi-Threading), HTML, Oracle Web logic server, Eclipse, MySQL, JUnit.\nGolan Technologies – Newyork                                                \t\t          Jun’09 - Sep’11 \nRole: Java Developer\nGolan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression. \nResponsibilities:\nInvolved in the analysis, design, implementation, and testing of the project.\nDeveloped UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface.\nImplemented the end-to-end functionality of the client requirement during the development phase.\nImplemented the functionality of mapping entities to the database using Hibernate.\nWritten SQL queries involved in the JDBC connection in accordance with the business logic.\nPerformed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results.\nActively participated in client meetings and taking the inputs for the additional functionality.\nInvolved in fixing bugs and unit testing with test cases using JUnit.\nEnvironment: J2EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL",
  "cleaned_text": "Candidate Sr. Hadoop Developer Phone Email ------------------------------------------------------------------------------------------------------------------------------------------ PROFESSIONAL SUMMARY Over years of experience including years of Big Data Ecosystem related technologies with full project development, implementation and deployment. Strong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie. Strong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. In-depth understanding of MapReduce Framework and Spark execution model. Worked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching. Strong experience working with both batch and real-time processing using Spark framework. Expertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API s. Hands on experience in installing, configuring and deploying Hadoop distributions in cloud environments Amazon Web Services . Expertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API s. Worked on building real time data workflows using Kafka, Spark streaming and HBase. Worked extensively on Hive for building complex data analytical applications. Very good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance. Used custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. Having knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters. Experienced in Cluster coordination services through zookeeper. Strong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats. Worked with Sqoop to move import export data from a relational database into Hadoop. Experience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions. Extensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts. Designed and implemented Hive and Pig UDF s using Java for evaluation, filtering, loading and storing of data. Experienced in job workflow scheduling and monitoring tools like Oozie. Well versed with UNIX and Linux command line and shell script. Adequate knowledge and working experience with agile methodology. TECHNICAL SKILLS EDUCATION Bachelor of Technology in Computer Science Engineering WORK EXPERIENCE Cigna Bloomfield, Connecticut Jul Present Role Hadoop Spark Developer Responsibilities Developed Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data. Developed highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement Data pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data. Developed Spark jobs and Hive Jobs to summarize and transform data. Used Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns. Involved in converting Hive queries into Spark transformations using Spark Data Frames in Scala. Automated creation and termination of AWS EMR clusters using AWS, java sdk. Built real time data pipelines by developing Kafka producers and spark streaming applications for consuming. Ingested syslog messages to Kafka. Worked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals. Handled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark. Having knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters. Exported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team. Experienced in cluster coordination services through Zookeeper. Installed, tested and deployed monitoring solutions with Splunk services. Used Hive to analyze the partitioned and bucketed data and computed various metrics for reporting. Developed Hive scripts in Hive QL to de-normalize and aggregate the data. Scheduled and executed workflows in Oozie to run various jobs. Designing & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases. Environment Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase. Qualcomm -- San Diego, CA Dec Jun Role Hadoop Spark Developer Responsibilities Part of Big Data Center of Excellence CoE , responsible for designing and building enterprise data analytics platform. Worked with respective business units in understanding the scope of the analytics requirements. Performed core ETL transformations in Spark. Automated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics. Created end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data. Developed end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala. Implemented Spark utilizing Spark-SQL heavily for faster development, and processing of data. Exploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode. Handled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables. Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Wrapper developed in Python for instantiating multithreaded application and running with other applications. Analyzed the data by performing Hive queries Hive QL and running Pig scripts Pig Latin to study customer behavior. Data warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse. Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting. Created components like Hive UDFs for missing functionality in HIVE for analytics. Worked on various performance optimizations like using distributed cache for small datasets, Partition, Bucketing in Hive and Map Side joins. Created Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly. Automated creation and termination of AWS EMR clusters using AWS, java sdk. Environment AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper. Hortonworks McKesson - Alpharetta, GA Dec Nov Hadoop Spark Developer Responsibilities Developed multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S buckets on daily basis. Created various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users. Implemented batch processing of jobs using Spark Scala API. Developed Sqoop scripts to import export data from Oracle to HDFS and into Hive tables. Stored the data in columnar formats using Hive. Involved building and managing NoSQL Database models using HBase. Worked in Spark to read the data from Hive and write it to Hbase. Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries. Worked with multiple file formats like Avro, Sequence, Parquet and Orc. Converted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data. Loaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications. Worked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame. Implemented business logic in Hive and written UDFs to process the data for analysis. Used Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs. Addressing the issues occurring due to the huge volume of data and transitions. Designed, documented operational problems by following standards and procedures using JIRA. Environment Java, Hadoop . . , Map Reduce , Spark, Unix, Pig . . , Hive . . , Linux, Sqoop . . , Flume . . , Eclipse, AWS EC , and Cloudera CDH . American Home Shield - Memphis, TN Dec Nov Role Hadoop Developer Responsibilities Migrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS. Mainly worked on Hive queries to categorize data of different claims. Involved in loading data from LINUX file system to HDFS Written customized Hive UDFs in Java where the functionality is too complex. Implemented Partitioning, Dynamic Partitions, Buckets in HIVE. Designing and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets. Generate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector. Responsible to manage the test data coming from different sources Reviewing peer table creation in Hive, data loading and queries. Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers. Monitored System health and logs and respond accordingly to any warning or failure conditions. Gained experience in managing and reviewing Hadoop log files. Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Involved unit testing, interface testing, system testing and user acceptance testing of the workflow tool. Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Environment Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH , Oracle, MySQL. Protective Life - Edina, MN Oct - Nov Role Java Developer Responsibilities Implemented a Web based Application using Servlets, JSP, spring, JDBC, XML. Involved in writing Spring Configuration XML file that contains declarations and other dependent objects declarations. Used hibernate to connect to Database to create the DAO layer. Developed Application Framework using Model-View-Controller using the technology Spring. Used HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages Multilayer Applications construction using Open JPA, HTML , Spring MVC. Annotated Spring Architecture Spring Beans Implemented UNIX shell scripts to migrate various data files to S&P ratings repository Implemented smooth pagination capability using JSP to remove existing pagination utility Worked on Geo API to provide geological access capability to S&P.com site. Involved in Agile process to streamline development process with iterative development. Code reviews and Managing the CVS Repository. Prepare builds for DEV and UAT environments. Participating in the regular team meetings sprint planning meetings, user story review meetings etc. Involved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool. Environment JDK . , XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log J, CSS Style Sheets, Apache Tomcat, J EE, Maven Accenture USA Oct Sep Role Java Developer Responsibilities Involved in Requirements analysis, design, and development and testing. Involved in setting up the different roles & maintained authentication to the application. Designed, deployed and tested Multi-tier application using the Java technologies. Involved in front end development using JSP, HTML & CSS. Implemented the Application using Servlets Deployed the application on Oracle Web logic server Implemented Multithreading concepts in java classes to avoid deadlocking. Used MySQL database to store data and execute SQL queries on the backend. Prepared and Maintained test environment. Tested the application before going live to production. Documented and communicated test result to the team lead on daily basis. Involved in weekly meeting with team leads and manager to discuss the issues and status of the projects. Environment J EE Java, JSP, JDBC, Multi-Threading , HTML, Oracle Web logic server, Eclipse, MySQL, JUnit. Golan Technologies Newyork Jun - Sep Role Java Developer Golan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression. Responsibilities Involved in the analysis, design, implementation, and testing of the project. Developed UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface. Implemented the end-to-end functionality of the client requirement during the development phase. Implemented the functionality of mapping entities to the database using Hibernate. Written SQL queries involved in the JDBC connection in accordance with the business logic. Performed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results. Actively participated in client meetings and taking the inputs for the additional functionality. Involved in fixing bugs and unit testing with test cases using JUnit. Environment J EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL",
  "preprocessed_text": "candidate sr. hadoop developer -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- professional summary year experience including year big data ecosystem related technology full project development implementation deployment strong experience working various hadoop ecosystem component like map reduce hdfs hive sqoop pig flume oozie strong knowledge architecture distributed system parallel processing framework in-depth understanding mapreduce framework spark execution model worked extensively fine-tuning long running spark application utilize better parallelism executor memory caching strong experience working batch real-time processing using spark framework expertise developing production ready spark application utilizing spark-core data frame spark-sql spark-ml spark-streaming api s. hand experience installing configuring deploying hadoop distribution cloud environment amazon web service expertise developing production ready spark application utilizing spark-core data frame spark-sql spark-ml spark-streaming api s. worked building real time data workflow using kafka spark streaming hbase worked extensively hive building complex data analytical application good understanding partition bucketing concept hive designed managed external table hive optimize performance used custom serdes like regex serde json serde csv serde etc. hive handle multiple format data knowledge apache ambari platform securing managing monitoring hadoop cluster experienced cluster coordination service zookeeper strong experience using different columnar file format like avro rcfile orc parquet format worked sqoop move import export data relational database hadoop experience working hadoop cluster using cloudera amazon emr hortonworks distribution extensive experience performing etl structured semi-structured data using pig latin script designed implemented hive pig udf using java evaluation filtering loading storing data experienced job workflow scheduling monitoring tool like oozie well versed unix linux command line shell script adequate knowledge working experience agile methodology technical skill education bachelor technology computer science engineering work experience cigna bloomfield connecticut jul present role hadoop spark developer responsibility developed spark application using scala utilizing data frame spark sql api faster processing data developed highly optimized spark application perform various data cleansing validation transformation summarization activity according requirement data pipeline consists spark hive sqoop custom built input adapter ingest transform analyze operational data developed spark job hive job summarize transform data used spark interactive query processing streaming data integration nosql database hbase cassandra interactive access pattern involved converting hive query spark transformation using spark data frame scala automated creation termination aws emr cluster using aws java sdk built real time data pipeline developing kafka producer spark streaming application consuming ingested syslog message kafka worked apache airflow schedule single sometimes complex chain task depend regular interval handled importing data relational database hdfs using sqoop performing transformation using hive spark knowledge apache ambari platform securing managing monitoring hadoop cluster exported processed data relational database using sqoop visualize generate report bi team experienced cluster coordination service zookeeper installed tested deployed monitoring solution splunk service used hive analyze partitioned bucketed data computed various metric reporting developed hive script hive ql de-normalize aggregate data scheduled executed workflow oozie run various job designing creating etl job talend load huge volume data cassandra hadoop ecosystem relational database environment hadoop spark hive java scala maven impala oozie oracle ambari tableau unix hortonworks apache airflow kafka zookeeper sqoop cassandra talend splunk hbase qualcomm -- san diego ca dec jun role hadoop spark developer responsibility part big data center excellence coe responsible designing building enterprise data analytics platform worked respective business unit understanding scope analytics requirement performed core etl transformation spark automated data pipeline involve data ingestion data cleansing data preparation data analytics created end end spark application using scala perform various data cleansing validation transformation summarization activity user behavioral data developed end-to-end data pipeline using ftp adaptor spark hive impala implemented spark utilizing spark-sql heavily faster development processing data exploring spark improving performance optimization existing job hadoop using spark-sql data frame running yarn mode handled importing enterprise data different data source hdfs using sqoop performing transformation using hive map reduce loading data hbase table collecting aggregating large amount log data using flume staging data hdfs analysis wrapper developed python instantiating multithreaded application running application analyzed data performing hive query hive ql running pig script pig latin study customer behavior data warehousing experience design development testing implementation support enterprise data warehouse used hive analyze partitioned bucketed data compute various metric reporting created component like hive udfs missing functionality hive analytics worked various performance optimization like using distributed cache small datasets partition bucketing hive map side join created oozie workflow coordinator automate data pipeline daily weekly monthly automated creation termination aws emr cluster using aws java sdk environment aws emr hadoop spark hive sqoop hbase unix talend pig linux java scala python ambari zookeeper hortonworks mckesson alpharetta ga dec nov hadoop spark developer responsibility developed multithreaded java based input adaptor ingesting click stream data external source like ftp server bucket daily basis created various spark application using scala perform various enrichment click stream data combined enterprise data user implemented batch processing job using spark scala api developed sqoop script import export data oracle hdfs hive table stored data columnar format using hive involved building managing nosql database model using hbase worked spark read data hive write hbase optimized hive table using optimization technique like partition bucketing provide better performance hive ql query worked multiple file format like avro sequence parquet orc converted existing mapreduce program spark application handling semi structured data like json file apache log file custom log data loaded final processed data hbase table allow downstream application team build rich data driven application worked team improve performance optimization existing algorithm hadoop using spark spark -sql data frame implemented business logic hive written udfs process data analysis used oozie define workflow coordinate execution spark hive sqoop job addressing issue occurring due huge volume data transition designed documented operational problem following standard procedure using jira environment java hadoop map reduce spark unix pig hive linux sqoop flume eclipse aws ec cloudera cdh american home shield memphis tn dec nov role hadoop developer responsibility migrated needed data mysql hdfs using sqoop importing various format flat file hdfs mainly worked hive query categorize data different claim involved loading data linux file system hdfs written customized hive udfs java functionality complex implemented partitioning dynamic partition bucket hive designing creating hive external table using shared meta-store instead derby partitioning dynamic partitioning bucket generate final reporting data using tableau testing connecting corresponding hive table using hive odbc connector responsible manage test data coming different source reviewing peer table creation hive data loading query weekly meeting technical collaborator active participation code review session senior junior developer monitored system health log respond accordingly warning failure condition gained experience managing reviewing hadoop log file involved scheduling oozie workflow engine run multiple hive pig job involved unit testing interface testing system testing user acceptance testing workflow tool created maintained technical documentation launching hadoop cluster executing hive query pig script environment apache hadoop hdfs hive map reduce core java pig sqoop cloudera cdh oracle mysql protective life edina mn oct nov role java developer responsibility implemented web based application using servlets jsp spring jdbc xml involved writing spring configuration xml file contains declaration dependent object declaration used hibernate connect database create dao layer developed application framework using model-view-controller using technology spring used html xhtml xml xslt xpath jsp tag library develop view page multilayer application construction using open jpa html spring mvc annotated spring architecture spring bean implemented unix shell script migrate various data file rating repository implemented smooth pagination capability using jsp remove existing pagination utility worked geo api provide geological access capability p.com site involved agile process streamline development process iterative development code review managing cv repository prepare build dev uat environment participating regular team meeting sprint planning meeting user story review meeting etc involved preparing high low level design doc uml diagram using microsoft visio tool environment jdk xml html xhtml jsp spring dao oracle express edition apache ant cv junit unix log cs style sheet apache tomcat ee maven accenture usa oct sep role java developer responsibility involved requirement analysis design development testing involved setting different role maintained authentication application designed deployed tested multi-tier application using java technology involved front end development using jsp html cs implemented application using servlets deployed application oracle web logic server implemented multithreading concept java class avoid deadlocking used mysql database store data execute sql query backend prepared maintained test environment tested application going live production documented communicated test result team lead daily basis involved weekly meeting team lead manager discus issue status project environment ee java jsp jdbc multi-threading html oracle web logic server eclipse mysql junit golan technology newyork jun sep role java developer golan technology range turnkey solution custom client-driven solution variety product category including website development platform based application demand intelligence business insight generation smart site ability provide unified user experience consistent messaging website across globe driving favorable brand impression responsibility involved analysis design implementation testing project developed ui using html javascript cs jsp interactive cross browser functionality complex user interface implemented end-to-end functionality client requirement development phase implemented functionality mapping entity database using hibernate written sql query involved jdbc connection accordance business logic performed various level unit testing entire application using test case included preparation detail documentation result actively participated client meeting taking input additional functionality involved fixing bug unit testing test case using junit environment ee spring hibernate javascript cs servlets mysql",
  "statistics": {
    "word_count": 2386,
    "unique_word_count": 739,
    "avg_word_length": 5.091366303436714,
    "stopword_count": 466
  },
  "metadata": {
    "filename": "Candidate23 Hadoop developer.docx",
    "file_size": 40572,
    "processing_time": null,
    "processing_timestamp": "2025-09-09T23:20:58.498007",
    "input_filename": "Candidate23 Hadoop developer.json"
  }
}